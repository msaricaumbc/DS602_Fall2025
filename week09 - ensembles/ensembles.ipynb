{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging, Boosting, and Ensembles\n",
    "\n",
    "## Agenda\n",
    "- Majority voting. \n",
    "- Bagging. \n",
    "- Boosting.  \n",
    "\n",
    "## Resources\n",
    "[XGBoost - optimized version of Gradient Boosting](https://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "# Ensemble Learning\n",
    "\n",
    "Models we have discussed previously:\n",
    "- Linear regression. \n",
    "- Logistic regression.  \n",
    "- Decision trees.  \n",
    "- k-Nearest neighbors.  \n",
    "- Support vector machines.  \n",
    "\n",
    "Sometimes these models are too simple (even tuned) to provide a model accurate enough to be worth putting into production, especially compute and infrastructure requirements.\n",
    "\n",
    "Another approach is ensemble learning - instead of trying to make a single model that has sufficient performance, maybe using multiple models and combining the results in some way will lead to better overall performance.\n",
    "\n",
    "This will generally follow Python Machine Learning 3rd Edition, Chapter 7.\n",
    "\n",
    "<img src='./diagrams/modelstacking.png' style='width: 500px'>\n",
    "\n",
    "[Image source: SAS Blog](https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b>Why bother? </b> A set of classifiers will often have better predictive performance than the individual members. </div>\n",
    "\n",
    "Think of a chorus of young students. Many will sing too low or too high, but on average they sound better together than individually. \n",
    "\n",
    "### Goal\n",
    "> Combine different classifers into a meta-classifier that has better generalization performance than each individual classifier alone.\n",
    "<br><br>Machine Learning with Python 3rd Edition, Page 223.\n",
    "\n",
    "Ask a panel of medical experts about health issues. For the common topics, they will have a tendency to agree. For rarer topics, the specialists, which there are fewer, will be more knowledgeable. The non-specialists opinions for those rare topics will probably be all over the place, but the specialist opinions will likely be in agreement.\n",
    "\n",
    "Sometimes these are also referred to as **meta-models.**\n",
    "\n",
    "The most common underlying models for these are weak decision trees, sometimes stumps, sometimes referred to as **weak learners**.  \n",
    "- Computationally pretty fast.  \n",
    "- Trees aren't identical and will be better than random guessing.  \n",
    "- Can obtain high accuracy with many of these independent trees, similar in concept to random forests.  \n",
    "- We aren't restricted to trees though, any classifier (regression) can be used to create ensembles.  \n",
    "\n",
    "### Common methods\n",
    "- Majority voting. \n",
    "- Bagging. \n",
    "- Boosting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Voting\n",
    "> Really talking about the plurality or mode. Majority doesn't generalize for multi-class problems.\n",
    "\n",
    "<img src='./diagrams/unaminity-majority-plurity.png' style='width: 500px'>\n",
    "\n",
    "[Image source: Machine Learning with Python 3rd Edition, Figure 7.1](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch07)\n",
    "\n",
    " </div> <div class=\"alert alert-block alert-success\"><b>Random Forests: </b> We've already encountered an ensemble model, random forests. Recall a random forest is a collection of individual decision trees, with the prediction being the plurality class predicted from the $C$ individual trees. </div>\n",
    "\n",
    "<img src='./diagrams/majorityvoting.png' style='width: 500px'>\n",
    "\n",
    "[Image source: Machine Learning with Python 3rd Edition, Figure 7.2](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch07)\n",
    "\n",
    "- $C$ different classification algorithms can be fit.  \n",
    "- These can include different models, different parameter settings, different feature processing, ...   \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Warning: </b> While these can be useful, for larger datasets and models that are computationally intensive by themselves, ensembles can be very expensive. </div>\n",
    "\n",
    "### Predicting the label ($mode=plurality$)\n",
    "$$\n",
    "\\hat{y} = mode(C_1(x), C_2(x), \\dots, C_m(x))\n",
    "$$\n",
    "\n",
    "### Rationale\n",
    "Assuming we have $n$ binary classifiers that are independent and the error rates are uncorrelated, the errors can be expressed as a probability mass function of a binomial distribution.\n",
    "\n",
    "$$\n",
    "P(y\\geq k) = \\sum_k^n {n \\choose k}\\epsilon^k(1-\\epsilon)^{n-k}=\\epsilon_{ensemble}\n",
    "$$\n",
    "\n",
    "i.e., compute the probability that the prediction is wrong. For 11 classifers ($n$) with error rates of 0.25, with a majority being 6 ($k$):\n",
    "$$\n",
    "P(y\\geq k) = \\sum_6^{11} {11 \\choose k}0.25^k(1-0.25)^{11-k}=0.034\n",
    "$$\n",
    "\n",
    "> Assuming the classifiers are (1) independent with (2) uncorrelated errors; the error rate with the ensemble is much lower than the individual error rates ($0.034 \\lt 0.25$).\n",
    "\n",
    "### Does it always work?\n",
    "No. In the class of binary classifiers, the classifiers have to do better than random guessing. [The below code is from page 226-227 of Machine Learning with Python](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch07)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "import math\n",
    "\n",
    "def ensemble_errors(n_classifier, error):\n",
    "    k_start = int(math.ceil(n_classifier / 2.))\n",
    "    probs = [comb(n_classifier, k) * (error ** k) * (1 - error)**(n_classifier - k)\n",
    "            for k in range(k_start, n_classifier+1)\n",
    "            ]\n",
    "    return sum(probs)\n",
    "\n",
    "example_ensemble_error = ensemble_errors(n_classifier=11, error=0.25)\n",
    "print(f'Error rate: {example_ensemble_error:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "error_range = np.arange(0.0, 1.01, 0.01)\n",
    "ens_errors = [ensemble_errors(n_classifier=11, error=error) for error in error_range]\n",
    "\n",
    "plt.plot(error_range, ens_errors, label='Ensemble Error', color='blue', linewidth=2)\n",
    "plt.plot(error_range, error_range, label='Base Error', linestyle='--', color='orange')\n",
    "plt.fill_between(error_range[:51], ens_errors[:51], error_range[:51], color='green', alpha=0.5)\n",
    "plt.xlabel('Base Error')\n",
    "plt.ylabel('Base/Ensemble Error')\n",
    "plt.legend(['Ensemble Errors', 'Base Estimator Errors', 'Area of Benefit'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Classifiers\n",
    "There is an extension that is implemented in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) that allows you to weight each classifier with an arbitrary weight.\n",
    "\n",
    "$$\n",
    "\\hat{y}=argmax{_t} \\sum_j^k w_j\\chi_A(C_j(x)=i)\n",
    "$$\n",
    "\n",
    "$w_j$ is a weight associated with the base classifier, $C_j$; $\\hat{y}$ is the predicted label; $A$ is the set of unique class labels; $\\chi_A$ is the indicator function that determines the class within the classifier.\n",
    "\n",
    "If you had $3$ classifiers and $w_{j's}=\\in (0.6, 0.2, 0.2)$, you would effectively be counting the first classifier's label $3$ times, the second classifier once, and the third classifier once.\n",
    "\n",
    "$$\n",
    "\\{C_1=1, C_2=0, C_3=0 \\} \\rightarrow  (1,1,1,0,0)\n",
    "$$\n",
    "\n",
    "For the probabilities (`predict_proba`) it is the weighted average of the individual probabilties:\n",
    "\n",
    "$$\n",
    "\\hat{y}=argmax{_t} \\sum_j^k w_jp_{i,j}\n",
    "$$\n",
    "\n",
    "Assuming we have $3$ binary classifiers, returning the following probabilities for an example:\n",
    "\n",
    "$$\n",
    "C_1(x) \\rightarrow [0.9, 0.1], C_2(x) \\rightarrow [0.8, 0.2], C_3(x) \\rightarrow [0.4, 0.6]\n",
    "$$\n",
    "\n",
    "Using the same weights, we would have:\n",
    "$$\n",
    "p(i_0|x) = 0.6\\times0.9 + 0.2\\times0.8 + 0.2\\times0.4 = 0.78\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(i_1|x) = 0.6\\times0.1 + 0.2\\times0.2 + 0.2\\times0.6 = 0.22\n",
    "$$\n",
    "\n",
    "And the overall result is:\n",
    "$$\\hat{y} = argmax{_t}[p(i_0|x),p(i_1|x)] = argmax{_t}[0.78, 0.22]= 0$$\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Not all probabilties are created equal: </b> Although we can weight the probabilities, they may not be calculated the same way. Recall decision tree probabilties are essentially the class distributions on the leafs and other models produce probabilities from probability mass functions. </div>\n",
    " \n",
    " ### Regression\n",
    " For regression, it is a bit simpler. Weight the individual $\\hat{y}$ values to obtain a weighted average estimate and recalcuate the $r^2$ or other scoring metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Majority Voting on `iris`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[50:, [1,2]], iris.target[50:,]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "print(f'Training examples: {X_train.shape[0]:,}')\n",
    "print(f'Test examples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression,  \n",
    "- Decision tree, and  \n",
    "- k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, solver='lbfgs', random_state=0)\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=1)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "\n",
    "pipe1 = Pipeline([('scaler', StandardScaler()),('logreg', clf1)])\n",
    "pipe2 = Pipeline([('scaler', StandardScaler()),('tree', clf2)])\n",
    "pipe3 = Pipeline([('scaler', StandardScaler()),('knn', clf3)])\n",
    "\n",
    "labs = ['Logistic Regression', 'Decision Tree', 'k-Nearest Neighbors']\n",
    "clfs = [pipe1, pipe2, pipe3]\n",
    "clfs = zip(labs, clfs)\n",
    "\n",
    "for lab, clf in clfs:\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(f'ROC AUC {scores.mean():.2f} (+/- {scores.std():.2f}) [{lab}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VotingClassifier from scikit-learn.\n",
    "- Set `voting=soft` to use the probabilities to inform the class prediction. `voting=hard` will use the mode of the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ems = [('lr', pipe1),('dt', pipe2),('knn', pipe3)]\n",
    "clf4 = VotingClassifier(estimators= ems, weights=None, voting='soft')\n",
    "\n",
    "scores = cross_val_score(estimator=clf4, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "# print(scores)\n",
    "print(f'ROC AUC {scores.mean():.2f} (+/- {scores.std():.2f}) [Ensemble]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Higher area under the curve, with smaller variations between the folds! That was on the training though, need to evaluate on the test data.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf4.fit(X_test, y_test)\n",
    "y_pred_prob = clf4.predict_proba(X_test) # <== REMEMBER! we need probabilities for ROC stores\n",
    "roc_auc_score(y_true=y_test, y_score=y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- We can tune a VotingClassifier the same we way tune individual ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'lr__logreg__C':[0.001, 0.1, 1, 10], \n",
    "          'dt__tree__max_depth': [1,2,3], \n",
    "          'knn__knn__n_neighbors': [1,2,3]\n",
    "         }\n",
    "\n",
    "vc_gs = GridSearchCV(estimator=clf4, param_grid=params, scoring='roc_auc', refit=True)\n",
    "vc_gs = vc_gs.fit(X_train, y_train)\n",
    "vc_gs_train_score = vc_gs.best_score_\n",
    "vc_gs_score = vc_gs.score(X_test, y_test)\n",
    "\n",
    "print(f'VotingClassifier Train ROC AUC: {vc_gs_train_score}')\n",
    "print(f'VotingClassifier Test ROC AUC: {vc_gs_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = vc_gs.predict_proba(X_test) # <== REMEMBER! we need probabilities for ROC stores\n",
    "roc_auc_score(y_true=y_test, y_score=y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (also known as Bootstrap Aggregating)\n",
    "- Another ensemble learning technique, similar to the VotingClassifier.  \n",
    "- Instead of using the same dataset for training, bootstrap samples are drawn.  \n",
    "- Aggregation should reduce bias and variation in our results.  \n",
    "\n",
    "<img src='./diagrams/bootstrap-samples.png' style='width: 500px'>\n",
    "\n",
    "[Image source: Machine Learning with Python 3rd Edition, Figure 7.6](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch07)\n",
    "\n",
    "> This can improve accuracy and decrease overfitting when models are unstable. [See Breiman's paper on bagging.](https://www.stat.berkeley.edu/~breiman/bagging.pdf)\n",
    "\n",
    "> Bagging won't help with bias or underfitting models. These are commonly used with unpruned decision trees, which by themselves are very prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression as Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, solver='lbfgs', random_state=0)\n",
    "pipe1 = Pipeline([('scaler', StandardScaler()),('logreg', clf1)])\n",
    "\n",
    "params = {'logreg__C':[0.001, 0.1, 1, 10]}\n",
    "\n",
    "lrcv = GridSearchCV(pipe1, param_grid=params, cv=10, scoring='roc_auc')\n",
    "lrcv = lrcv.fit(X_train, y_train)\n",
    "lrcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average deviation on test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.cv_results_['std_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "params = {\n",
    "    'max_depth':[1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "dtcv = GridSearchCV(dt, param_grid=params, cv=10, scoring='roc_auc')\n",
    "dtcv = dtcv.fit(X_train, y_train)\n",
    "print('score', dtcv.score(X_test, y_test))\n",
    "print('std test score', dtcv.cv_results_['std_test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "bclf = BaggingClassifier(dt)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 15, 20],\n",
    "    'estimator__max_depth':[1, 2, 4, 6], \n",
    "    'max_samples':[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "blrcv = GridSearchCV(bclf, param_grid=params, cv=10, scoring='roc_auc')\n",
    "blrcv = blrcv.fit(X_train, y_train)\n",
    "blrcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blrcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average deviation of test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blrcv.cv_results_['std_test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `iris` is pretty stable, so no benefit, but you can try bagging if overfitting seems to be an issue. However, we do see lower standard deviations in the test scores across the folds - less bias in bagging.\n",
    "\n",
    "#### Takeaways\n",
    "- Performance was about the same with bagging.  \n",
    "- Variance likely lower in our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Boosting (AdaBoost)\n",
    "- Boosting refers to ensembles that combine weak learners into a strong learner. \n",
    "- Generally, the weak decision trees are used as the learner.  \n",
    "- Generally, we sequentially train weak models and the subsequent try to correct the prior model's mistakes.  \n",
    "- AdaBoost and Gradient Boosting are the most popular.  \n",
    "- Since the learners are dependent on previous learners, this is not able to be parallelized and won't scale as well as bagging.  \n",
    "\n",
    "### Original Process\n",
    "- Draw random subset of examples, $d_1$, without replacement, and train a weak learner, $C_1$.  \n",
    "- Draw second sample, $d_2$, and add 50% of samples that were misclassified in $C_1$ to a second weak learner, $C_2$.  \n",
    "- Find examples, $d_3$ that $C_1$ and $C_2$ disagreed on and train another weak learner, $C_3$.  \n",
    "- Combined $C_1$, $C_2$, and $C_3$ via majority voting.  \n",
    "\n",
    "### New Process\n",
    "Newer process work much in the same way as above, except all examples are used in each learner and those misclassified are weighted heavier.\n",
    "\n",
    "In the image below:\n",
    "1. Represents the initial learner with all examples, equal-weighted.  \n",
    "2. Assign larger weight to misclassified examples from first learner (bigger circles) and train another learner.  \n",
    "3. Assign yet larger weights to misclassified examples from second learner and train another learner.  \n",
    "4. Combine three learners by weighted majority vote.  \n",
    "\n",
    "<img src='./diagrams/adaboost.png' style='width: 600px'>\n",
    "\n",
    "[Image source: Machine Learning with Python 3rd Edition, Figure 7.9](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch07)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo Code\n",
    "\n",
    "- Set weight vector, $w$, to uniform weights, $\\sum_iw_i=1$.  \n",
    "- For $j$ in $m$ boosting rounds:\n",
    "    - Train a weak learner: $C_j=train(X,y,w)$.  \n",
    "    - Predict labels: $\\hat{y}=predict(C_j,X)$.  \n",
    "    - Compute weighted error rate: $\\epsilon=w(\\hat{y}\\ne y)$.  \n",
    "    - Compute coefficient: $\\alpha_j=0.5log\\frac{1-\\epsilon}{\\epsilon}$  \n",
    "    - Update weights: $w:=w\\times exp(-\\alpha_j x \\hat{y} \\times y)$  \n",
    "    - Normalize weights: $w:=w/\\sum_iw_i$  \n",
    "- Compute final predictions:\n",
    "$$\n",
    "\\hat{y}=\\sum{_{j=1}^{m}}(\\alpha_j \\times predict(C_j,X)>0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine Example\n",
    "Data from [UCI's Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/ensembles/wine.data', header=None)\n",
    "\n",
    "wineCols = ['Class label', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash',\n",
    "           'Magnesium', 'Total Phenols', 'Flavanoid', 'Nonflavanoid Phenols',\n",
    "           'Proanthocyanins', 'Color Intensity', 'Hue', 'OD280-OD315', 'Proline']\n",
    "\n",
    "wine.columns = wineCols\n",
    "\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['Class label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wineFeatures = [x for x in wineCols if x != 'Class label']\n",
    "wineX = wine[wineFeatures]\n",
    "winey = wine['Class label']\n",
    "\n",
    "wX_train, wX_test, wy_train, wy_test = train_test_split(wineX, winey, test_size=0.3)\n",
    "\n",
    "print(f'Training examples: {wX_train.shape[0]}')\n",
    "print(f'Test examples: {wX_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine performance on Decision Tree Stumps ($depth=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=1)\n",
    "tree = tree.fit(wX_train, wy_train)\n",
    "tree_train_predict = tree.predict(wX_train)\n",
    "tree_test_predict = tree.predict(wX_test)\n",
    "\n",
    "tree_train_score = accuracy_score(wy_train, tree_train_predict)\n",
    "tree_test_score = accuracy_score(wy_test, tree_test_predict)\n",
    "\n",
    "print(f'Tree Training Score: {tree_train_score:.2%}')\n",
    "print(f'Tree Test Score: {tree_test_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use AdaBoost, with the Stump Decision Tree as the base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)\n",
    "ada = ada.fit(wX_train.values, wy_train)\n",
    "ada_train_predict = ada.predict(wX_train.values)\n",
    "ada_test_predict = ada.predict(wX_test.values)\n",
    "\n",
    "ada_train_score = accuracy_score(wy_train, ada_train_predict)\n",
    "ada_test_score = accuracy_score(wy_test, ada_test_predict)\n",
    "\n",
    "print(f'AdaBoost Training Score: {ada_train_score:.2%}')\n",
    "print(f'AdaBoost Test Score: {ada_test_score:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AdaBoost perfectly assigns all the training data. Probably overfitting - see the drop in the test score. Want would to run through cross-validation to ensure stability of those high scores. The absolute gap is bigger in the AdaBoost classifier, so we introduced additional model bias.\n",
    "\n",
    "**AdaBoost Hyperparameters to Tune:**  \n",
    "- Number of estimators. \n",
    "- Learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "Similar to AdaBoost, in that it sequentially adds learners to an ensemble. Instead of changing weights after iteration, this tries to fit the new predictor to the residual errors made by the previous learner.\n",
    "\n",
    "Example from [Hands on Machine Learning with Scikit-Learn, Keras & TensorFlow, pages 204-205](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb)\n",
    "\n",
    "Generate quadratic data with a little noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "plt.plot(X,y, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an initial weak Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate residuals and use those as the target variable in another weak Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate residuals and use those as the target variable in another weak Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New predictions can be made by summing up the residuals from the three trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn Equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt = gbrt.fit(X, y)\n",
    "\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Gradient Boost from scikit-learn\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity of Learning Rates and Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=0.01)\n",
    "gbrt = gbrt.fit(X, y)\n",
    "\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Gradient Boost from scikit-learn ($\\\\alpha=0.01$, $estimators=3$)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=0.1)\n",
    "gbrt = gbrt.fit(X, y)\n",
    "\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Gradient Boost from scikit-learn ($\\\\alpha=0.1$, $estimators=3$)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=300, learning_rate=0.01)\n",
    "gbrt = gbrt.fit(X, y)\n",
    "\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Gradient Boost from scikit-learn ($\\\\alpha=0.01$, $estimators=300$)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=300, learning_rate=0.1)\n",
    "gbrt = gbrt.fit(X, y)\n",
    "\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Gradient Boost from scikit-learn ($\\\\alpha=0.1$, $estimators=300$)\", fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate affects the contributions from each tree. Lower values will require more trees, but generalization will be better (shrinkage).  \n",
    "- Increasing the number of trees can lead to overfitting. Use early stopping to determine an optimal number of trees. Early stopping looks at the validation errors as the number of trees increase.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "min_error = np.min(errors)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Error\", fontsize=16)\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More tangible example using customer churn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/ensembles/telco.csv')\n",
    "\n",
    "tel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Churn distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.Churn.value_counts().plot.barh()\n",
    "plt.title('Customer Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unbalanced, but not as severe as when seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cardinality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.select_dtypes('object').nunique().sort_values().plot.barh()\n",
    "plt.title('Unique Values of Objects')\n",
    "plt.show()\n",
    "\n",
    "tel.select_dtypes('object').nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- customerID is something we won't want to use for modeling.  \n",
    "- TotalCharges might have an incorrect type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.TotalCharges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel['TotalCharges'] = pd.to_numeric(tel['TotalCharges'], errors='coerce')\n",
    "tel['TotalCharges'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.select_dtypes(['int64','float64']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SeniorCitizen looks like a dummy. \n",
    "- tenure and MonthlyCharges seem to be truncated to some degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Will need an imputer; missing values likely caused by the type conversion.  \n",
    "- If there were a substantial amount of NAs due to the type conversion, we may have to handle it more elegantly, especially if the data generation process will allow this field to come in as objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- Baseline with logistic regression.  \n",
    "- Create a straight decision tree.  \n",
    "- AdaBoost.  \n",
    "- GradientBoost.  \n",
    "\n",
    "We'll use `accuracy` as a metric, but you could make the arguement recall may be what we'd want to optimize for since we could proactively reach-out to those customers and try to get them to stay on the service with discounts or other incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = tel.dtypes[tel.dtypes != 'object'].index.tolist()\n",
    "\n",
    "cats = tel.dtypes[tel.dtypes == 'object'].index.tolist()\n",
    "cats = [x for x in cats if x not in ['Churn', 'customerID']]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([('impute_missing', SimpleImputer(strategy='median')),\n",
    "                           ('standardize_num', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "cat_pipeline = Pipeline([('impute_missing_cats', SimpleImputer(strategy='most_frequent')),\n",
    "                          ('create_dummies_cats', OneHotEncoder(handle_unknown='ignore', drop='first'))])\n",
    "\n",
    "processing_pipeline = ColumnTransformer(transformers=[('proc_numeric', num_pipeline, nums),\n",
    "                                                      ('create_dummies', cat_pipeline, cats)])\n",
    "\n",
    "print('Pipeline Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.where(tel.Churn == 'Yes', 1, 0)\n",
    "\n",
    "tX_train, tX_test, ty_train, ty_test = train_test_split(tel[nums+cats],\n",
    "                                                       y,\n",
    "                                                       test_size=0.2\n",
    "                                                       )\n",
    "\n",
    "print(f'Training examples: {tX_train.shape[0]:,}')\n",
    "print(f'Test examples: {tX_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "p1 = Pipeline([('processing', processing_pipeline),\n",
    "             ('lr', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "params = {'lr__C': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "lr_gscv = GridSearchCV(p1, param_grid=params, cv=10, scoring='accuracy', refit=True)\n",
    "lr_gscv = lr_gscv.fit(tX_train, ty_train)\n",
    "\n",
    "print(f'Validation score: {lr_gscv.best_score_:.2%}')\n",
    "\n",
    "lr_pred = lr_gscv.predict(tX_test)\n",
    "\n",
    "print(f'Test score: {lr_gscv.score(tX_test, ty_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "p2 = Pipeline([('processing', processing_pipeline),\n",
    "             ('dt', DecisionTreeClassifier())])\n",
    "\n",
    "params = {'dt__max_depth': [1, 5, 10, 15, 25],\n",
    "         'dt__min_samples_split': [3, 10, 15]}\n",
    "\n",
    "dt_gscv = GridSearchCV(p2, param_grid=params, cv=10, scoring='accuracy', refit=True)\n",
    "dt_gscv = dt_gscv.fit(tX_train, ty_train)\n",
    "\n",
    "print(f'Validation score: {dt_gscv.best_score_:.2%}')\n",
    "\n",
    "dt_pred = dt_gscv.predict(tX_test)\n",
    "\n",
    "print(f'Test score: {dt_gscv.score(tX_test, ty_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# multi-step pipelines don't play as nice with AdaBoost\n",
    "xt = processing_pipeline.fit_transform(tX_train)\n",
    "\n",
    "params = {'estimator__max_depth': [1,2],\n",
    "         'n_estimators': [50, 100, 200, 400]\n",
    "         }\n",
    "\n",
    "#ABC = AdaBoostClassifier(base_estimator=p2)\n",
    "ABC = AdaBoostClassifier(DecisionTreeClassifier(), algorithm='SAMME')\n",
    "\n",
    "\n",
    "ad_gscv = GridSearchCV(ABC, param_grid = params, cv=10, scoring='accuracy')\n",
    "           \n",
    "ad_gscv = ad_gscv.fit(xt, ty_train)\n",
    "\n",
    "print(f'Validation score: {ad_gscv.best_score_:.2%}')\n",
    "\n",
    "xtt = processing_pipeline.transform(tX_test)\n",
    "\n",
    "ad_pred = ad_gscv.predict(xtt)\n",
    "\n",
    "print(f'Test score: {ad_gscv.score(xtt, ty_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "p3 = Pipeline([('processing', processing_pipeline),\n",
    "             ('gb', GradientBoostingClassifier())])\n",
    "\n",
    "params = {'gb__max_depth': [1,2,3],\n",
    "          'gb__n_estimators': [50, 100, 200]\n",
    "         }\n",
    "\n",
    "gb_gscv = GridSearchCV(p3, param_grid = params, cv=10, scoring='accuracy')\n",
    "           \n",
    "gb_gscv = gb_gscv.fit(tX_train, ty_train)\n",
    "\n",
    "print(f'Validation score: {gb_gscv.best_score_:.2%}')\n",
    "\n",
    "gb_pred = gb_gscv.predict(tX_test)\n",
    "\n",
    "print(f'Test score: {gb_gscv.score(tX_test, ty_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Logistic Regression: {lr_gscv.score(tX_test, ty_test):.2%}')\n",
    "print(f'Decision Tree: {dt_gscv.score(tX_test, ty_test):.2%}')\n",
    "print(f'AdaBoost: {ad_gscv.score(xtt, ty_test):.2%}')\n",
    "print(f'Gradient Boosting: {gb_gscv.score(tX_test, ty_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensembles can reduce bias and potentially increase performance.  \n",
    "- Usually a trade-off with computational complexity.  \n",
    "- Plenty of examples in real life where an ensemble performed the best, but it was impractical to implement.\n",
    "- Sometimes, but not always, helps performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "- [XGBoost - optimized version of Gradient Boosting](https://xgboost.readthedocs.io/en/latest/)  \n",
    "- [Netflix Model](https://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)\n",
    "- [AdaBoost - youtube](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n",
    "- [GradientBoosting - youtube](https://www.youtube.com/watch?v=3CC4N4z3GJc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
